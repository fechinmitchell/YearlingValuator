# -*- coding: utf-8 -*-
"""YearlingValuator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HKA0rtVPG1hZMMY_snNnKrUTdm32RZL2
"""

!nvidia-smi

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Input, concatenate

# Unzip the YearlingPictures.zip file
!unzip -o "/content/YearlingPictures.zip" -d "/content/YearlingPictures"

# Correct the path to the CSV file
csv_file_path = '/content/YearlingPictures/content/YearlingPictures/horse_data.csv'

# Load the data
data = pd.read_csv(csv_file_path)

# Debugging: Check the contents of the 'lot_number' and 'session_number' columns
print("Unique values in 'lot_number' before conversion:", data['lot_number'].unique())
print("Unique values in 'session_number' before conversion:", data['session_number'].unique())

# Convert 'lot_number' and 'session_number' to numeric, coercing errors to NaN
data['lot_number'] = pd.to_numeric(data['lot_number'], errors='coerce')
data['session_number'] = pd.to_numeric(data['session_number'], errors='coerce')

# Debugging: Check how many non-NaN values are there after conversion
print("Non-NaN values in 'lot_number' after conversion:", data['lot_number'].notna().sum())
print("Non-NaN values in 'session_number' after conversion:", data['session_number'].notna().sum())

# If you have non-NaN values left after conversion, proceed with the scaler
if data['lot_number'].notna().sum() > 0 and data['session_number'].notna().sum() > 0:
    numerical_features = ['lot_number', 'session_number']
    scaler = StandardScaler()
    data[numerical_features] = scaler.fit_transform(data[numerical_features])
else:
    print("All values in 'lot_number' and/or 'session_number' are NaN. Check your data extraction process.")

# Correct the path for image preprocessing function
def preprocess_image(image_url):
    image_filename = os.path.basename(image_url)
    image_path = os.path.join('/content/YearlingPictures/content/YearlingPictures', image_filename)
    image = load_img(image_path, target_size=(224, 224))
    image = img_to_array(image)
    image = preprocess_input(image)
    return image

# Check if 'image_url' column exists in data
if 'image_url' in data.columns:
    data['processed_image'] = data['image_url'].apply(preprocess_image)
else:
    raise ValueError("The 'image_url' column is missing from the data")


# One-hot encode the categorical features
categorical_features = ['color', 'sex', 'sire', 'dam', 'damsire', 'bonus_scheme', 'buyer_name', 'status', 'stable']
one_hot_encoder = OneHotEncoder()
encoded_categorical = one_hot_encoder.fit_transform(data[categorical_features]).toarray()

# One-hot encode the 'session_number' column since it contains categorical data
data['session_status'] = data['session_number']  # Rename for clarity
session_status_encoder = OneHotEncoder()
session_status_encoded = session_status_encoder.fit_transform(data[['session_status']]).toarray()

# Convert 'lot_number' to numeric, coercing errors to NaN
data['lot_number'] = pd.to_numeric(data['lot_number'], errors='coerce')

# Drop rows with NaNs in 'lot_number' column if they can't be converted to numbers
data.dropna(subset=['lot_number'], inplace=True)

# Now apply the scaler only to 'lot_number'
numerical_features = ['lot_number']  # Only one numeric feature in this case
scaler = StandardScaler()
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Prepare the target variable (price) by extracting numeric values and handling missing values
data['price'] = pd.to_numeric(data['price'].str.extract(r'(\d+\.?\d*)')[0], errors='coerce')
data.dropna(subset=['price'], inplace=True)  # Drop rows where price could not be converted to numeric

# Split into features and target
X_images = np.stack(data['processed_image'].values)
X_categorical = encoded_categorical
X_numerical = data[numerical_features].values
y = data['price'].values

# Split the data into training and test sets
X_train_images, X_test_images, X_train_categorical, X_test_categorical, X_train_numerical, X_test_numerical, y_train, y_test = train_test_split(
    X_images, X_categorical, X_numerical, y, test_size=0.2, random_state=42
)

# Load a pre-trained ResNet50 model for image processing
base_model = ResNet50(weights='imagenet', include_top=False)
base_model.trainable = False  # Freeze the model

# Image processing branch
image_input = Input(shape=(224, 224, 3))
x_image = base_model(image_input)
x_image = Flatten()(x_image)

# Categorical features branch
categorical_input = Input(shape=(X_categorical.shape[1],))
x_categorical = Dense(64, activation='relu')(categorical_input)

# Numerical features branch
numerical_input = Input(shape=(X_numerical.shape[1],))
x_numerical = Dense(64, activation='relu')(numerical_input)

# Combine branches
combined = concatenate([x_image, x_categorical, x_numerical])
combined = Dense(64, activation='relu')(combined)
output = Dense(1)(combined)  # Regression output

model = Model(inputs=[image_input, categorical_input, numerical_input], outputs=output)

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(
    [X_train_images, X_train_categorical, X_train_numerical],
    y_train,
    validation_split=0.1,
    epochs=10
)

# Evaluate the model
model.evaluate([X_test_images, X_test_categorical, X_test_numerical], y_test)